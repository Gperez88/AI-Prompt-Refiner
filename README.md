# AI Prompt Refiner

Refina y optimiza tus prompts para asistentes de codificaci√≥n de IA (como Cursor, VS Code AI, Copilot) directamente dentro de VS Code.

## üöÄ Caracter√≠sticas

* **Refinamiento por IA**: Reescribe tus prompts para que sean claros, sin ambig√ºedades y optimizados para modelos de lenguaje (LLMs).
* **Experiencia Zero-Config**: ¬°√ösalo inmediatamente! Incluye modelos gratuitos preconfigurados que no requieren API Key.
* **M√∫ltiples Proveedores**:
  * **Modelos P√∫blicos (Gratis)**: Acceso instant√°neo a GPT-4o Mini, LLaMA 3.1 y Claude 3 Haiku sin configuraci√≥n.
  * **Groq**: Acceso ultra r√°pido a LLaMA 3, Mixtral y Gemma.
  * **Google Gemini**: Soporte nativo para Gemini 1.5 Flash y Pro.
  * **HuggingFace**: Acceso a modelos abiertos como Qwen 2.5, BLOOM y Mistral.
  * **OpenAI**: Soporte para GPT-4o y GPT-4o-mini.
  * **GitHub Marketplace**: Acceso a modelos como GPT-4o y LLaMA 3.1 70B usando tu cuenta de GitHub.
  * **Ollama**: Ejecuci√≥n local para m√°xima privacidad.
* **Vista de Diferencias (Diff View)**: Compara instant√°neamente tu prompt original con la versi√≥n refinada.
* **Barra de Estado Integradada**: Visualiza el modelo activo y c√°mbialo r√°pidamente desde la barra inferior de VS Code.
* **Modo Estricto**: Fuerza respuestas funcionales sin rellenos conversacionales.

## üìñ Gu√≠a de Uso

### 1. Refinar un Prompt

1. Selecciona el texto que deseas mejorar en cualquier editor de VS Code.
2. Haz clic derecho y selecciona **Prompt Refiner: Refine Selection** o usa la paleta de comandos (`Ctrl+Shift+P`).
3. Se abrir√° una vista de diferencias comparando tu prompt original (izquierda) con el optimizado (derecha).

### 2. Cambiar de Modelo R√°pidamente

Tienes tres formas de cambiar el modelo o proveedor:

* **Barra de Estado**: Haz clic en el icono `$(zap)` o `$(sparkle)` en la parte inferior derecha. El icono `$(zap)` indica que est√°s usando un modelo gratuito "Zero-Config".
* **Atajo de Teclado**: Presiona `Ctrl + Alt + M` (o `Cmd + Alt + M` en Mac).
* **Comando**: Ejecuta `Prompt Refiner: Select Model` desde la paleta de comandos.

### 3. Configuraci√≥n Inicial

* **Modelos P√∫blicos**: No requieren configuraci√≥n. La extensi√≥n est√° lista para usar desde el primer segundo.
* **Proveedores Cloud**: La primera vez que uses un proveedor privado (como Groq, Gemini o OpenAI), la extensi√≥n te solicitar√° tu **API Key**, que se guardar√° de forma segura.

## ‚öôÔ∏è Configuraci√≥n (Settings)

Puedes ajustar el comportamiento en los ajustes de VS Code (`Ctrl + ,`):

* `promptRefiner.provider`: Selecciona el proveedor (`public`, `groq`, `gemini`, `openai`, `huggingface`, `ollama`).
* `promptRefiner.model`: Especifica el ID del modelo (ej. `gpt-4o-mini`, `llama-3.1-70b`).
* `promptRefiner.strictMode`: Activa/Desactiva el formato de salida estricto (predeterminado: `true`).
* `promptRefiner.ollamaEndpoint`: URL para tu instancia local de Ollama (predeterminado: `http://localhost:11434`).

## üì¶ Desarrollo y Compilaci√≥n

Si deseas compilar la extensi√≥n manualmente y generar el archivo `.vsix`:

1.  Ejecuta el script automatizado:
    ```bash
    .\build.bat
    ```
2.  El script instalar√° las dependencias, compilar√° el c√≥digo TypeScript y generar√° el archivo `prompt-refiner-x.x.x.vsix` en la ra√≠z del proyecto.

## üõ†Ô∏è Soluci√≥n de Problemas (Troubleshooting)

Si encuentras errores durante el refinamiento, consulta estas soluciones comunes:

| Error / S√≠ntoma | Causa Probable | Soluci√≥n Sugerida |
| :--- | :--- | :--- |
| **"Challenge (418)"** en modelos DDG | DuckDuckGo ha bloqueado temporalmente el acceso automatizado. | Cambia a un modelo de **HuggingFace Public** (ej. Mistral 7B) en la selecci√≥n de modelos. Son m√°s estables. |
| **"VQD missing"** o **"400 Bad Request"** | Cambio en la API p√∫blica o token de sesi√≥n expirado. | Intenta de nuevo. Si persiste, cambia a un modelo de HuggingFace o usa un proveedor con API Key. |
| **"Unauthorized (401)"** | La API Key configurada es incorrecta o ha expirado. | Usa el comando `Prompt Refiner: Set API Key` para actualizar tu clave para ese proveedor. |
| **"Models permission required"** (GitHub) | El token de GitHub no tiene el permiso de lectura necesario. | Aseg√∫rate de que el token tenga el permiso **"GitHub Models" (read-only)**. |
| **"Connection Refused"** en Ollama | Ollama no est√° corriendo o el endpoint es incorrecto. | Aseg√∫rate de que Ollama est√© abierto y ejecut√°ndose en `http://localhost:11434`. |
| **"Rate Limit Exceeded"** | Has realizado demasiadas peticiones en poco tiempo. | Espera unos minutos. Los modelos gratuitos tienen l√≠mites de uso m√°s estrictos. |
| **Resultado "ejecuta" el prompt** (en Ollama) | El modelo local confunde el refinamiento con una orden directa. | Aseg√∫rate de tener el modelo `llama3` o superior. Hemos optimizado la extensi√≥n para evitar esto, pero si persiste, intenta ser m√°s espec√≠fico en tu prompt original. |

> **Tip de Estabilidad**: Para una experiencia gratuita m√°s predecible y sin bloqueos, utiliza los modelos marcados como **(HF Public)** en el men√∫ de selecci√≥n de modelos.

> **Tip para Ollama**: Si usas modelos locales peque√±os (como Phi-3 o Gemma), es posible que no sigan el template perfectamente. Recomendamos `llama3` o `mistral` para mejores resultados de refinamiento.

## üõ†Ô∏è Requisitos

* **Sin Requisitos**: Para usar los modelos gratuitos del proveedor `public`.
* **API Key**: Necesaria solo para proveedores privados (Groq, Gemini, HF, OpenAI).
* **Ollama**: Debe estar ejecut√°ndose localmente si prefieres usar el proveedor local.

---
*Desarrollado para mejorar la productividad en ingenier√≠a de prompts.*
